{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ed83d3d-fabd-4ca8-96b6-1591a45b80b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def func(df, batchId):\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql.functions import col, row_number, trim, current_date\n",
    "    from delta.tables import DeltaTable\n",
    "\n",
    "    # Deduplicate by cst_id keeping the latest record\n",
    "    df = (\n",
    "        df.withColumn(\n",
    "            'dedup',\n",
    "            row_number().over(Window.partitionBy(\"cst_id\").orderBy(col(\"updated_at\").desc()))\n",
    "        )\n",
    "        .filter((col('dedup') == 1) & col('cst_id').isNotNull())\n",
    "        .drop('dedup')\n",
    "    )\n",
    "\n",
    "    # Create a temporary view for SQL transformation\n",
    "    df.createOrReplaceTempView('cust_info')\n",
    "\n",
    "    # Transform data\n",
    "    df = spark.sql('''\n",
    "        SELECT \n",
    "            cst_id,\n",
    "            cst_key,\n",
    "            TRIM(cst_firstname) AS cst_firstname,\n",
    "            TRIM(cst_lastname) AS cst_lastname,\n",
    "            CASE \n",
    "                WHEN cst_marital_status = \"M\" THEN \"Married\"\n",
    "                WHEN cst_marital_status = \"S\" THEN \"Single\"\n",
    "                ELSE \"N/A\"\n",
    "            END AS cst_marital_status,\n",
    "            CASE  \n",
    "                WHEN cst_gndr = \"M\" THEN \"Male\"\n",
    "                WHEN cst_gndr = \"F\" THEN \"Female\"\n",
    "                ELSE \"N/A\"\n",
    "            END AS cst_gender,\n",
    "            cst_create_date,\n",
    "            current_date() as modified_date\n",
    "        FROM cust_info\n",
    "    ''')\n",
    "\n",
    "    # Define Delta path\n",
    "    delta_path = 'abfss://silver@storagetemp.dfs.core.windows.net/cust_info/data/'\n",
    "\n",
    "    # Write or merge data into Delta table\n",
    "    if not DeltaTable.isDeltaTable(spark, delta_path):\n",
    "        # First-time write\n",
    "        df.write.format('delta').mode('overwrite').save(delta_path)\n",
    "    else:\n",
    "        # Perform upsert (merge)\n",
    "        trg = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "        (\n",
    "            trg.alias('trg')\n",
    "            .merge(\n",
    "                df.alias('src'),\n",
    "                'trg.cst_id = src.cst_id'\n",
    "            )\n",
    "            .whenMatchedUpdateAll(condition='trg.updated_at < src.updated_at')\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "\n",
    "\n",
    "df1 = spark.readStream.format('cloudFiles') \\\n",
    "    .option('cloudFiles.format', 'parquet') \\\n",
    "    .option('cloudFiles.schemaLocation', 'abfss://silver@storagetemp.dfs.core.windows.net/cust_info/checkpoint') \\\n",
    "    .option('cloudFiles.schemaEvolutionMode', 'rescue') \\\n",
    "    .load('abfss://bronze@storagetemp.dfs.core.windows.net/cust_info/')\n",
    "\n",
    "\n",
    "df1.writeStream.foreachBatch(func) \\\n",
    "    .outputMode('append') \\\n",
    "    .option('checkpointLocation', 'abfss://silver@storagetemp.dfs.core.windows.net/cust_info/checkpoint') \\\n",
    "    .option('path', 'abfss://silver@storagetemp.dfs.core.windows.net/cust_info/data/')\\\n",
    "    .trigger(once=True) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d312eba8-dbbc-4a0c-937d-f2d3ff93f442",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def func(df, batchId):\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql.functions import col, row_number, trim\n",
    "    from delta.tables import DeltaTable\n",
    "\n",
    "    # Deduplicate by cid keeping the latest record\n",
    "    df = (\n",
    "        df.withColumn(\n",
    "            'dedup',\n",
    "            row_number().over(Window.partitionBy(\"cid\").orderBy(col(\"updated_at\").desc()))\n",
    "        )\n",
    "        .filter((col('dedup') == 1) & col('cid').isNotNull())\n",
    "        .drop('dedup')\n",
    "    )\n",
    "\n",
    "    # Create a temporary view for SQL transformation\n",
    "    df.createOrReplaceTempView('cust_az12')\n",
    "\n",
    "    # Transform data\n",
    "    df = spark.sql('''\n",
    "        SELECT\n",
    "\t\t\tCASE\n",
    "\t\t\t\tWHEN cid LIKE 'NAS%' THEN SUBSTRING(cid, 4, LEN(cid)) -- Remove 'NAS' prefix if present\n",
    "\t\t\t\tELSE cid\n",
    "\t\t\tEND AS cid, \n",
    "\t\t\tCASE\n",
    "\t\t\t\tWHEN bdate > current_date() THEN NULL\n",
    "\t\t\t\tELSE bdate\n",
    "\t\t\tEND AS bdate, -- Set future birthdates to NULL\n",
    "\t\t\tCASE\n",
    "\t\t\t\tWHEN UPPER(TRIM(gen)) IN ('F', 'FEMALE') THEN 'Female'\n",
    "\t\t\t\tWHEN UPPER(TRIM(gen)) IN ('M', 'MALE') THEN 'Male'\n",
    "\t\t\t\tELSE 'n/a'\n",
    "\t\t\tEND AS gen,\n",
    "            current_date() as modified_date\n",
    "        FROM cust_az12\n",
    "    ''')\n",
    "\n",
    "    # Define Delta path\n",
    "    delta_path = 'abfss://silver@storagetemp.dfs.core.windows.net/cust_az12/data/'\n",
    "\n",
    "    # Write or merge data into Delta table\n",
    "    if not DeltaTable.isDeltaTable(spark, delta_path):\n",
    "        # First-time write\n",
    "        df.write.format('delta').mode('overwrite').save(delta_path)\n",
    "    else:\n",
    "        # Perform upsert (merge)\n",
    "        trg = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "        (\n",
    "            trg.alias('trg')\n",
    "            .merge(\n",
    "                df.alias('src'),\n",
    "                'trg.cid = src.cid'\n",
    "            )\n",
    "            .whenMatchedUpdateAll(condition='trg.updated_at < src.updated_at')\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "\n",
    "\n",
    "cust_az12 = spark.readStream.format('cloudFiles') \\\n",
    "    .option('cloudFiles.format', 'parquet') \\\n",
    "    .option('cloudFiles.schemaLocation', 'abfss://silver@storagetemp.dfs.core.windows.net/cust_az12/checkpoint') \\\n",
    "    .option('cloudFiles.schemaEvolutionMode', 'rescue') \\\n",
    "    .load('abfss://bronze@storagetemp.dfs.core.windows.net/CUST_AZ12/')\n",
    "\n",
    "\n",
    "cust_az12.writeStream.foreachBatch(func) \\\n",
    "    .outputMode('append') \\\n",
    "    .option('checkpointLocation', 'abfss://silver@storagetemp.dfs.core.windows.net/cust_az12/checkpoint') \\\n",
    "    .option('path', 'abfss://silver@storagetemp.dfs.core.windows.net/cust_az12/data/')\\\n",
    "    .trigger(once=True) \\\n",
    "    .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22afa08b-f6ae-45a8-b71e-191a8f665ec6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def func(df, batchId):\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql.functions import col, row_number, trim\n",
    "    from delta.tables import DeltaTable\n",
    "\n",
    "    # Deduplicate by cid keeping the latest record\n",
    "    df = (\n",
    "        df.withColumn(\n",
    "            'dedup',\n",
    "            row_number().over(Window.partitionBy(\"cid\").orderBy(col(\"updated_at\").desc()))\n",
    "        )\n",
    "        .filter((col('dedup') == 1) & col('cid').isNotNull())\n",
    "        .drop('dedup')\n",
    "    )\n",
    "\n",
    "    # Create a temporary view for SQL transformation\n",
    "    df.createOrReplaceTempView('loc_a101')\n",
    "\n",
    "    # Transform data\n",
    "    df = spark.sql('''\n",
    "        SELECT\n",
    "\t\t\tREPLACE(cid, '-', '') AS cid, \n",
    "\t\t\tCASE\n",
    "\t\t\t\tWHEN TRIM(cntry) = 'DE' THEN 'Germany'\n",
    "\t\t\t\tWHEN TRIM(cntry) IN ('US', 'USA') THEN 'United States'\n",
    "\t\t\t\tWHEN TRIM(cntry) = '' OR cntry IS NULL THEN 'n/a'\n",
    "\t\t\t\tELSE TRIM(cntry)\n",
    "\t\t\tEND AS cntry,\n",
    "            current_date() as modified_date\n",
    "        FROM loc_a101\n",
    "    ''')\n",
    "\n",
    "    # Define Delta path\n",
    "    delta_path = 'abfss://silver@storagetemp.dfs.core.windows.net/loc_a101/data/'\n",
    "\n",
    "    # Write or merge data into Delta table\n",
    "    if not DeltaTable.isDeltaTable(spark, delta_path):\n",
    "        # First-time write\n",
    "        df.write.format('delta').mode('overwrite').save(delta_path)\n",
    "    else:\n",
    "        # Perform upsert (merge)\n",
    "        trg = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "        (\n",
    "            trg.alias('trg')\n",
    "            .merge(\n",
    "                df.alias('src'),\n",
    "                'trg.cid = src.cid'\n",
    "            )\n",
    "            .whenMatchedUpdateAll(condition='trg.updated_at < src.updated_at')\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "\n",
    "\n",
    "loc_a101 = spark.readStream.format('cloudFiles') \\\n",
    "    .option('cloudFiles.format', 'parquet') \\\n",
    "    .option('cloudFiles.schemaLocation', 'abfss://silver@storagetemp.dfs.core.windows.net/loc_a101/checkpoint') \\\n",
    "    .option('cloudFiles.schemaEvolutionMode', 'rescue') \\\n",
    "    .load('abfss://bronze@storagetemp.dfs.core.windows.net/LOC_A101/')\n",
    "\n",
    "\n",
    "loc_a101.writeStream.foreachBatch(func) \\\n",
    "    .outputMode('append') \\\n",
    "    .option('checkpointLocation', 'abfss://silver@storagetemp.dfs.core.windows.net/loc_a101/checkpoint') \\\n",
    "    .option('path', 'abfss://silver@storagetemp.dfs.core.windows.net/loc_a101/data/')\\\n",
    "    .trigger(once=True) \\\n",
    "    .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "541b7d0e-ac6d-4c19-9511-633c8e5588eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def func(df, batchId):\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql.functions import col, row_number, trim\n",
    "    from delta.tables import DeltaTable\n",
    "\n",
    "    # Deduplicate by prd_id keeping the latest record\n",
    "    df = (\n",
    "        df.withColumn(\n",
    "            'dedup',\n",
    "            row_number().over(Window.partitionBy(\"prd_id\").orderBy(col(\"updated_at\").desc()))\n",
    "        )\n",
    "        .filter((col('dedup') == 1) & col('prd_id').isNotNull())\n",
    "        .drop('dedup')\n",
    "    )\n",
    "\n",
    "    # Create a temporary view for SQL transformation\n",
    "    df.createOrReplaceTempView('prd_info')\n",
    "\n",
    "    # Transform data\n",
    "    df = spark.sql('''\n",
    "        SELECT\n",
    "\t\t\tprd_id,\n",
    "\t\t\tREPLACE(SUBSTRING(prd_key, 1, 5), '-', '_') AS cat_id, -- Extract category ID\n",
    "\t\t\tSUBSTRING(prd_key, 7, LENGTH(prd_key)) AS prd_key,        -- Extract product key\n",
    "\t\t\tprd_nm,\n",
    "\t\t\tCOALESCE(prd_cost, 0) AS prd_cost,\n",
    "\t\t\tCASE \n",
    "\t\t\t\tWHEN UPPER(TRIM(prd_line)) = 'M' THEN 'Mountain'\n",
    "\t\t\t\tWHEN UPPER(TRIM(prd_line)) = 'R' THEN 'Road'\n",
    "\t\t\t\tWHEN UPPER(TRIM(prd_line)) = 'S' THEN 'Other Sales'\n",
    "\t\t\t\tWHEN UPPER(TRIM(prd_line)) = 'T' THEN 'Touring'\n",
    "\t\t\t\tELSE 'n/a'\n",
    "\t\t\tEND AS prd_line, -- Map product line codes to descriptive values\n",
    "\t\t\tCAST(prd_start_dt AS DATE) AS prd_start_dt,\n",
    "\t\t\tCAST(\n",
    "\t\t\t\tLEAD(prd_start_dt) OVER (PARTITION BY prd_key ORDER BY prd_start_dt) - 1 \n",
    "\t\t\t\tAS DATE\n",
    "\t\t\t) AS prd_end_dt,\n",
    "            current_date() as modified_date\n",
    "        FROM prd_info\n",
    "    ''')\n",
    "\n",
    "    # Define Delta path\n",
    "    delta_path = 'abfss://silver@storagetemp.dfs.core.windows.net/prd_info/data/'\n",
    "\n",
    "    # Write or merge data into Delta table\n",
    "    if not DeltaTable.isDeltaTable(spark, delta_path):\n",
    "        # First-time write\n",
    "        df.write.format('delta').mode('overwrite').save(delta_path)\n",
    "    else:\n",
    "        # Perform upsert (merge)\n",
    "        trg = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "        (\n",
    "            trg.alias('trg')\n",
    "            .merge(\n",
    "                df.alias('src'),\n",
    "                'trg.prd_id = src.prd_id'\n",
    "            )\n",
    "            .whenMatchedUpdateAll(condition='trg.updated_at < src.updated_at')\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "\n",
    "\n",
    "prd_info = spark.readStream.format('cloudFiles') \\\n",
    "    .option('cloudFiles.format', 'parquet') \\\n",
    "    .option('cloudFiles.schemaLocation', 'abfss://silver@storagetemp.dfs.core.windows.net/prd_info/checkpoint') \\\n",
    "    .option('cloudFiles.schemaEvolutionMode', 'rescue') \\\n",
    "    .load('abfss://bronze@storagetemp.dfs.core.windows.net/prd_info/')\n",
    "\n",
    "\n",
    "prd_info.writeStream.foreachBatch(func) \\\n",
    "    .outputMode('append') \\\n",
    "    .option('checkpointLocation', 'abfss://silver@storagetemp.dfs.core.windows.net/prd_info/checkpoint') \\\n",
    "    .option('path', 'abfss://silver@storagetemp.dfs.core.windows.net/prd_info/data/')\\\n",
    "    .trigger(once=True) \\\n",
    "    .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f5e683a-16e8-419c-aea8-078bc32b8b71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def func(df, batchId):\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql.functions import col, row_number, trim\n",
    "    from delta.tables import DeltaTable\n",
    "\n",
    "    # Deduplicate by id keeping the latest record\n",
    "    df = (\n",
    "        df.withColumn(\n",
    "            'dedup',\n",
    "            row_number().over(Window.partitionBy(\"id\").orderBy(col(\"updated_at\").desc()))\n",
    "        )\n",
    "        .filter((col('dedup') == 1) & col('id').isNotNull())\n",
    "        .drop('dedup')\n",
    "    )\n",
    "\n",
    "    # Create a temporary view for SQL transformation\n",
    "    df.createOrReplaceTempView('px_cat_g1v2')\n",
    "\n",
    "    # Transform data\n",
    "    df = spark.sql('''\n",
    "        SELECT\n",
    "\t\t\tid,\n",
    "\t\t\tcat,\n",
    "\t\t\tsubcat,\n",
    "\t\t\tmaintenance,\n",
    "            current_date() as modified_date\n",
    "        FROM px_cat_g1v2\n",
    "    ''')\n",
    "\n",
    "    # Define Delta path\n",
    "    delta_path = 'abfss://silver@storagetemp.dfs.core.windows.net/px_cat_g1v2/data/'\n",
    "\n",
    "    # Write or merge data into Delta table\n",
    "    if not DeltaTable.isDeltaTable(spark, delta_path):\n",
    "        # First-time write\n",
    "        df.write.format('delta').mode('overwrite').save(delta_path)\n",
    "    else:\n",
    "        # Perform upsert (merge)\n",
    "        trg = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "        (\n",
    "            trg.alias('trg')\n",
    "            .merge(\n",
    "                df.alias('src'),\n",
    "                'trg.id = src.id'\n",
    "            )\n",
    "            .whenMatchedUpdateAll(condition='trg.updated_at < src.updated_at')\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "\n",
    "\n",
    "px_cat_g1v2 = spark.readStream.format('cloudFiles') \\\n",
    "    .option('cloudFiles.format', 'parquet') \\\n",
    "    .option('cloudFiles.schemaLocation', 'abfss://silver@storagetemp.dfs.core.windows.net/px_cat_g1v2/checkpoint') \\\n",
    "    .option('cloudFiles.schemaEvolutionMode', 'rescue') \\\n",
    "    .load('abfss://bronze@storagetemp.dfs.core.windows.net/PX_CAT_G1V2/')\n",
    "\n",
    "\n",
    "px_cat_g1v2.writeStream.foreachBatch(func) \\\n",
    "    .outputMode('append') \\\n",
    "    .option('checkpointLocation', 'abfss://silver@storagetemp.dfs.core.windows.net/px_cat_g1v2/checkpoint') \\\n",
    "    .option('path', 'abfss://silver@storagetemp.dfs.core.windows.net/px_cat_g1v2/data/')\\\n",
    "    .trigger(once=True) \\\n",
    "    .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87f47abc-5a1c-44a3-be79-b39e361d7e22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def func(sales_details, batchsls_ord_num):\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql.functions import col, row_number\n",
    "    from delta.tables import DeltaTable\n",
    "\n",
    "    # Deduplicate by sls_ord_num keeping the latest record\n",
    "    sales_details = (\n",
    "        sales_details.withColumn(\n",
    "            'dedup',\n",
    "            row_number().over(Window.partitionBy(\"sls_ord_num\").orderBy(col(\"updated_at\").desc()))\n",
    "        )\n",
    "        .filter((col('dedup') == 1) & col('sls_ord_num').isNotNull())\n",
    "        .drop('dedup')\n",
    "    )\n",
    "\n",
    "    # Create a temporary view for SQL transformation\n",
    "    sales_details.createOrReplaceTempView('sales_details')\n",
    "\n",
    "    # Transform data\n",
    "    sales_details = spark.sql('''\n",
    "        SELECT \n",
    "            sls_ord_num,\n",
    "            sls_prd_key,\n",
    "            sls_cust_id,\n",
    "\n",
    "            CASE \n",
    "                WHEN sls_order_dt = 0 OR LENGTH(sls_order_dt) != 8 THEN NULL\n",
    "                ELSE TO_DATE(CAST(sls_order_dt AS STRING), 'yyyyMMdd')\n",
    "            END AS sls_order_dt,\n",
    "\n",
    "            CASE \n",
    "                WHEN sls_ship_dt = 0 OR LENGTH(sls_ship_dt) != 8 THEN NULL\n",
    "                ELSE TO_DATE(CAST(sls_ship_dt AS STRING), 'yyyyMMdd')\n",
    "            END AS sls_ship_dt,\n",
    "\n",
    "            CASE \n",
    "                WHEN sls_due_dt = 0 OR LENGTH(sls_due_dt) != 8 THEN NULL\n",
    "                ELSE TO_DATE(CAST(sls_due_dt AS STRING), 'yyyyMMdd')\n",
    "            END AS sls_due_dt,\n",
    "\n",
    "            CASE \n",
    "                WHEN sls_sales IS NULL \n",
    "                    OR sls_sales <= 0 \n",
    "                    OR sls_sales != sls_quantity * ABS(sls_price)\n",
    "                THEN sls_quantity * ABS(sls_price)\n",
    "                ELSE sls_sales\n",
    "            END AS sls_sales,\n",
    "\n",
    "            sls_quantity,\n",
    "            ROUND(\n",
    "                CASE \n",
    "                    WHEN sls_price IS NULL OR sls_price <= 0 THEN \n",
    "                        CASE WHEN sls_quantity != 0 THEN sls_sales / sls_quantity ELSE NULL END\n",
    "                    ELSE sls_price\n",
    "                END\n",
    "            , 2) AS sls_price,\n",
    "            current_date() as modified_date\n",
    "\n",
    "        FROM sales_details\n",
    "    ''')\n",
    "\n",
    "    # Define Delta path\n",
    "    delta_path = 'abfss://silver@storagetemp.dfs.core.windows.net/sales_details/data/'\n",
    "\n",
    "    # Write or merge data into Delta table\n",
    "    if not DeltaTable.isDeltaTable(spark, delta_path):\n",
    "        # First-time write\n",
    "        sales_details.write.format('delta').mode('overwrite').save(delta_path)\n",
    "    else:\n",
    "        # Perform upsert (merge)\n",
    "        trg = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "        (\n",
    "            trg.alias('trg')\n",
    "            .merge(\n",
    "                sales_details.alias('src'),\n",
    "                'trg.sls_ord_num = src.sls_ord_num'\n",
    "            )\n",
    "            .whenMatchedUpdateAll(condition='trg.updated_at < src.updated_at')\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "\n",
    "\n",
    "sales_details = spark.readStream.format('cloudFiles') \\\n",
    "    .option('cloudFiles.format', 'parquet') \\\n",
    "    .option('cloudFiles.schemaLocation', 'abfss://silver@storagetemp.dfs.core.windows.net/sales_details/checkpoint') \\\n",
    "    .option('cloudFiles.schemaEvolutionMode', 'rescue') \\\n",
    "    .load('abfss://bronze@storagetemp.dfs.core.windows.net/sales_details/')\n",
    "\n",
    "\n",
    "sales_details.writeStream.foreachBatch(func) \\\n",
    "    .outputMode('append') \\\n",
    "    .option('checkpointLocation', 'abfss://silver@storagetemp.dfs.core.windows.net/sales_details/checkpoint') \\\n",
    "    .option('path', 'abfss://silver@storagetemp.dfs.core.windows.net/sales_details/data/')\\\n",
    "    .trigger(once=True) \\\n",
    "    .start()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6515370801769094,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
